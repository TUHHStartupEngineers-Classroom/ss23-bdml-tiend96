---
title: "Session 6 - Challenge - Company Segmentation"
date: "7/24/2020"
output: 
    html_document:
        toc: TRUE
        theme: flatly
        highlight: tango
        code_folding: hide
        df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE
    )
```


# Libraries

Load the following libraries. 
```{r}

# Standard
library(tidyverse)

# Modeling
library(parsnip)

# Preprocessing & Sampling
library(recipes)
library(rsample)

# Modeling Error Metrics
library(yardstick)

# Plotting Decision Trees
library(rpart.plot)

library(forcats)

library(workflows)
```

# Modeling ----------------------------------------------------------------

```{r}
bike_orderlines_tbl <- readRDS("C:/Users/tiend/Documents/GitHub/ss23-bdml-tiend96/Challenge2/bike_orderlines.rds")
glimpse(bike_orderlines_tbl)

model_sales_tbl <- bike_orderlines_tbl %>%
    select(total_price, model, category_2, frame_material) %>%
    
    group_by(model, category_2, frame_material) %>%
    summarise(total_sales = sum(total_price)) %>%
    ungroup() %>%
    
    arrange(desc(total_sales))

model_sales_tbl %>%
    mutate(category_2 = as_factor(category_2) %>% 
               fct_reorder(total_sales, .fun = max) %>% 
               fct_rev()) %>%
    
    ggplot(aes(frame_material, total_sales)) +
    geom_violin() +
    geom_jitter(width = 0.1, alpha = 0.5, color = "#2c3e50") +
    #coord_flip() +
    facet_wrap(~ category_2) +
    scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = "M", accuracy = 0.1)) +
    tidyquant::theme_tq() +
    labs(
        title = "Total Sales for Each Model",
        x = "Frame Material", y = "Revenue"
    )
```
# Features

```{r}
bike_features_tbl <- readRDS("C:/Users/tiend/Documents/GitHub/ss23-bdml-tiend96/Challenge2/bike_features_tbl.rds")
glimpse(bike_features_tbl)

bike_features_tbl <- bike_features_tbl %>% 
    select(model:url, `Rear Derailleur`, `Shift Lever`) %>% 
    mutate(
      `shimano dura-ace`        = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano dura-ace ") %>% as.numeric(),
      `shimano ultegra`         = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano ultegra ") %>% as.numeric(),
      `shimano 105`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano 105 ") %>% as.numeric(),
      `shimano tiagra`          = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano tiagra ") %>% as.numeric(),
      `Shimano sora`            = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano sora") %>% as.numeric(),
      `shimano deore`           = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano deore(?! xt)") %>% as.numeric(),
      `shimano slx`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano slx") %>% as.numeric(),
      `shimano grx`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano grx") %>% as.numeric(),
      `Shimano xt`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano deore xt |shimano xt ") %>% as.numeric(),
      `Shimano xtr`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano xtr") %>% as.numeric(),
      `Shimano saint`           = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano saint") %>% as.numeric(),
      `SRAM red`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram red") %>% as.numeric(),
      `SRAM force`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram force") %>% as.numeric(),
      `SRAM rival`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram rival") %>% as.numeric(),
      `SRAM apex`               = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram apex") %>% as.numeric(),
      `SRAM xx1`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram xx1") %>% as.numeric(),
      `SRAM x01`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram x01|sram xo1") %>% as.numeric(),
      `SRAM gx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram gx") %>% as.numeric(),
      `SRAM nx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram nx") %>% as.numeric(),
      `SRAM sx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram sx") %>% as.numeric(),
      `SRAM sx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram sx") %>% as.numeric(),
      `Campagnolo potenza`      = `Rear Derailleur` %>% str_to_lower() %>% str_detect("campagnolo potenza") %>% as.numeric(),
      `Campagnolo super record` = `Rear Derailleur` %>% str_to_lower() %>% str_detect("campagnolo super record") %>% as.numeric(),
      `shimano nexus`           = `Shift Lever`     %>% str_to_lower() %>% str_detect("shimano nexus") %>% as.numeric(),
      `shimano alfine`          = `Shift Lever`     %>% str_to_lower() %>% str_detect("shimano alfine") %>% as.numeric()
    ) %>% 
  # Remove original columns  
  select(-c(`Rear Derailleur`, `Shift Lever`)) %>% 
  # Set all NAs to 0
  mutate_if(is.numeric, ~replace(., is.na(.), 0))

```
# Order and tidy the tibble

```{r}
# 2.0 TRAINING & TEST SETS ----
bike_features_tbl <- bike_features_tbl %>% 
  
  mutate(id = row_number()) %>% 
  
  select(id, everything(), -url)
```


# Splitting the Data

```{r}
bike_features_tbl %>% distinct(category_2)

# run both following commands at the same time
set.seed(seed = 1113)
split_obj <- rsample::initial_split(bike_features_tbl, prop   = 0.80, 
                                                       strata = "category_2")

# Check if testing contains all category_2 values
split_obj %>% training() %>% distinct(category_2)
split_obj %>% testing() %>% distinct(category_2)

# Assign training and test data
train_tbl <- training(split_obj)
test_tbl  <- testing(split_obj)

# We have to remove spaces and dashes from the column names
train_tbl <- train_tbl %>% set_names(str_replace_all(names(train_tbl), " |-", "_"))
test_tbl  <- test_tbl  %>% set_names(str_replace_all(names(test_tbl),  " |-", "_"))

train_tbl$category_2 <- factor(train_tbl$category_2, levels = unique(bike_features_tbl$category_2))
test_tbl$category_2 <- factor(test_tbl$category_2, levels = unique(bike_features_tbl$category_2))


#Deleted E-Road, because there is no E-Road in the training_Tbl. Thus, it can't be trained on. (diff. levels)
#Not Necessary, if used a recipe, due to "step_unknown()" function
#test_tbl <- test_tbl[test_tbl$category_2 != "E-Road", ]

bike_features_tbl


```

#Recipe : Challenge 2

# Step 1: Define the recipe
```{r}
bikes_rec <- 
  recipe(price ~ category_2 + frame_material, data = train_tbl) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_unknown(all_nominal())

```


# 3.0 LINEAR METHODS ----
# 3.1 LINEAR REGRESSION - NO ENGINEERED FEATURES ----
```{r}
# 3.1.1 Model ----

model_01_linear_lm_simple <- linear_reg(mode = "regression") %>%
    set_engine("lm") #%>%
 # fit(price ~ category_2 + frame_material, data = train_tbl)


```

```{r}
bikes_wflow <- workflow() %>%
  add_model(model_01_linear_lm_simple) %>%
  add_recipe(bikes_rec)
bikes_wflow

bikes_fit <-
  bikes_wflow %>%
  fit(data = train_tbl)



```
#Predicting with a Recipe
```{r}

bikes_pred <-
  predict(bikes_fit,test_tbl)
bikes_pred


```
#Not Necessary anymore, because the prediction is done differently with a recipe


model_01_linear_lm_simple %>%
  predict(new_data = test_tbl)





#Calculating the common metrics comparing a “truth” (price) to an “estimate” (.pred).

```{r}
#model_01_linear_lm_simple %>%
#  predict(new_data = test_tbl) %>%
bikes_pred %>%

    bind_cols(test_tbl %>% select(price)) %>%
    
    yardstick::metrics(truth = price, estimate = .pred)


```
#Extracting the Model of the Workflow

```{r}

model_01_linear_lm_simple <- bikes_fit %>% 
  pull_workflow_fit() #%>% 
 # tidy() 

```

# Model Explanation
# 3.1.2 Feature Importance ----
```{r}
#View(model_01_linear_lm_simple) # You will see the coefficients in the element "fit"

# tidy() function is applicable for objects with class "lm"
#model_01_linear_lm_simple %>% class()

model_01_linear_lm_simple %>%
  broom::tidy() %>%# NOT NECESSARY with a recipe, because while extracting the model, it gets tidied already
  arrange(p.value) %>%
  mutate(term = as_factor(term) %>% fct_rev()) %>%
  ggplot(aes(x = estimate, y = term)) +
  geom_point(color = "#2dc6d6", size = 3) +
  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = " €", prefix = "")),
                            size = 4, fill = "#272A36", color = "white") +
  scale_x_continuous(labels = scales::dollar_format(suffix = " €", prefix = "")) +
  labs(title = "Linear Regression: Feature Importance",
       subtitle = "Model 01: Simple lm Model") 

```
# 3.1.3 Function to Calculate Metrics Without Recipe ----


# Code we used earlier
model_01_linear_lm_simple %>%
    predict(new_data = test_tbl) %>%

    bind_cols(test_tbl %>% select(price)) %>%
    yardstick::metrics(truth = price, estimate = .pred)

# Generalized into a function
calc_metrics <- function(model, new_data = test_tbl) {

    model %>%
        predict(new_data = new_data) %>%

        bind_cols(new_data %>% select(price)) %>%
        yardstick::metrics(truth = price, estimate = .pred)

}

model_01_linear_lm_simple %>% calc_metrics(test_tbl)

#Function to Calculate Metrics With a Recipe
```{r}


calc_metrics <- function(model, new_data = test_tbl) {

    bikes_wflow <- workflow() %>%
      add_model(model) %>%
      add_recipe(bikes_rec)
      bikes_wflow
      
    bikes_fit <-
     bikes_wflow %>%
     fit(data = train_tbl)
    
    bikes_pred <-
     predict(bikes_fit,test_tbl)
     bikes_pred
     
    bikes_pred %>%
      bind_cols(test_tbl %>% select(price)) %>%
      yardstick::metrics(truth = price, estimate = .pred)
    
    model <- 
     pull_workflow_fit(bikes_fit) 
}

```
# 3.2 LINEAR REGRESSION - WITH ENGINEERED FEATURES ----
```{r}


# 3.2.1 Model ----
model_02_linear_lm_complex <- linear_reg("regression") %>%
    set_engine("lm")# %>%
    
    # This is going to be different. Remove unnecessary columns.
  #  fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))

```
#New Recipe (Preprocessing)

```{r}

bikes_rec <- 
  recipe(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender))) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_unknown(all_nominal())

model_02_linear_lm_complex <- calc_metrics(model_02_linear_lm_complex, test_tbl)

```
# 3.2.2 Feature importance ----
```{r}

model_02_linear_lm_complex %>%
  broom::tidy() %>%
  arrange(p.value) %>%
  mutate(term = as_factor(term) %>% fct_rev()) %>%
  
  ggplot(aes(x = estimate, y = term)) +
  geom_point(color = "#2dc6d6", size = 3) +
  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = " €", prefix = "")),
                            size = 4, fill = "#272A36", color = "white") +
  scale_x_continuous(labels = scales::dollar_format(suffix = " €", prefix = "")) +
  labs(title = "Linear Regression: Feature Importance",
       subtitle = "Model 02: Complex lm Model")

```
# 3.3 PENALIZED REGRESSION ----
```{r}
# 3.3.1 Model ----


model_03_linear_glmnet <- linear_reg(mode    = "regression", 
                                     penalty = 10, 
                                     mixture = 0.1) %>%
    set_engine("glmnet") 
   # fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))

#model_03_linear_glmnet %>% calc_metrics(test_tbl)

```
#New Recipe (Preprocessing)

```{r}

bikes_rec <- 
  recipe(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender))) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_unknown(all_nominal())

model_03_linear_glmnet <- calc_metrics(model_03_linear_glmnet, test_tbl)

```

```{r}
# 3.3.2 Feature Importance ----
model_03_linear_glmnet %>%
    broom::tidy() %>%
    #filter(lambda >= 10 & lambda < 11) %>%
    
    # No p value here
    arrange(desc(abs(estimate))) %>%
    mutate(term = as_factor(term) %>% fct_rev()) %>%

    ggplot(aes(x = estimate, y = term)) +
    geom_point() +
    ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1)),
                              size = 2) +
    scale_x_continuous(labels = scales::dollar_format()) +
    labs(title = "Linear Regression: Feature Importance",
         subtitle = "Model 03: GLMNET Model")


```

```{r}
# 4.0 TREE-BASED METHODS ----
# 4.1 DECISION TREES ----
# 4.1.1 Model ----

model_04_tree_decision_tree <- decision_tree(mode = "regression",
              
              # Set the values accordingly to get started
              cost_complexity = 0.001,
              tree_depth      = 5,
              min_n           = 7) %>%
              
    set_engine("rpart")
    #fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))

#model_04_tree_decision_tree %>% calc_metrics(test_tbl)
model_04_tree_decision_tree <- calc_metrics(model_04_tree_decision_tree, test_tbl)

```
# 4.1.2 Decision Tree Plot ----
```{r}


model_04_tree_decision_tree$fit %>%
    rpart.plot(roundint = FALSE)



# Optimze plot
model_04_tree_decision_tree$fit %>%
    rpart.plot(
        roundint = FALSE,
        type = 4,
        extra = 101, # see help page
        fallen.leaves = FALSE, # changes the angles from 90 to 45-degree
        cex = 0.6, # font size
        main = "Model 04: Decision Tree", # Adds title
        box.palette = "Blues",
        box.col = "lightblue"
        )

show.prp.palettes()

```
# 4.2 RANDOM FOREST ----
```{r}
# 4.2.1 Model: ranger ----

library(ranger)

# Remove rows with missing values in the "category_2" column
train_tbl_clean <- train_tbl %>% drop_na(category_2)

set.seed(1234)
model_05_rand_forest_ranger <- rand_forest(
    mode = "regression", mtry = 8, trees = 5000, min_n = 10
    ) %>%
    
    # Run ?ranger::ranger to play around with many arguments
    # We need to set importance to impurity to be able to explain the model in the next step
    set_engine("ranger", replace = TRUE, splitrule = "extratrees", importance = "impurity") 
  #  fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))

#model_05_rand_forest_ranger %>% calc_metrics(test_tbl)
model_05_rand_forest_ranger <- calc_metrics(model_05_rand_forest_ranger, test_tbl)
```
# 4.2.2 ranger: Feature Importance ----
```{r}



model_05_rand_forest_ranger$fit %>%
    ranger::importance() %>%
    enframe() %>%
    arrange(desc(value)) %>%
    mutate(name = as_factor(name) %>% fct_rev()) %>%

    ggplot(aes(value, name)) +
    geom_point() +
    labs(title = "ranger: Variable Importance",
         subtitle = "Model 05: Ranger Random Forest Model")
```
# 4.2.3 Model randomForest ----
```{r}


set.seed(1234)
model_06_rand_forest_randomForest <- rand_forest("regression") %>%
    set_engine("randomForest") #%>%
    
    # All character variables have to be changed to factor variables
   # fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)) %>% mutate_if(is.character, as_factor))

#model_06_rand_forest_randomForest %>% calc_metrics(test_tbl)

```
#New Recipe (Preprocessing)

```{r}

bikes_rec <- 
  recipe(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)) %>% mutate_if(is.character, as_factor)) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_unknown(all_nominal())

model_06_rand_forest_randomForest <- calc_metrics(model_06_rand_forest_randomForest, test_tbl)

```

# 4.2.4 randomForest: Feature Importance ----
```{r}

model_06_rand_forest_randomForest$fit %>%
    randomForest::importance() %>%
    # Convert matrix to tibble
    as_tibble(rownames = "name") %>%
    arrange(desc(IncNodePurity)) %>%
    mutate(name = as_factor(name) %>% fct_rev()) %>%

    ggplot(aes(IncNodePurity, name)) +
    geom_point() +
    labs(
        title = "randomForest: Variable Importance",
        subtitle = "Model 06: randomForest Model"
    )
```
# 4.3 XGBOOST ----
# 4.3.1 Model ----
```{r}

set.seed(1234)
model_07_boost_tree_xgboost <- boost_tree(
    mode = "regression",
    mtry = 30,
    learn_rate = 0.25,
    tree_depth = 7
    ) %>%
    set_engine("xgboost")
   # fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))
```
#New Recipe (Preprocessing)

```{r}

bikes_rec <- 
  recipe(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender))) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_unknown(all_nominal())

model_07_boost_tree_xgboost <- calc_metrics(model_07_boost_tree_xgboost, test_tbl)
#model_07_boost_tree_xgboost %>% calc_metrics(test_tbl)

```


# 4.3.2 Feature Importance ----
```{r}

model_07_boost_tree_xgboost$fit %>%
    xgboost::xgb.importance(model = .) %>%
    as_tibble() %>%
    arrange(desc(Gain)) %>%
    mutate(Feature = as_factor(Feature) %>% fct_rev()) %>%

    ggplot(aes(Gain, Feature)) +
    geom_point() +
    labs(
        title = "XGBoost: Variable Importance",
        subtitle = "Model 07: XGBoost Model"
    )

```
