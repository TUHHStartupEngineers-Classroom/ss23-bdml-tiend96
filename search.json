[
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\nThis is a .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#header-2",
    "href": "content/01_journal/01_tidyverse.html#header-2",
    "title": "Tidyverse",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html",
    "href": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html",
    "title": "Machine Learninng Fundamentals",
    "section": "",
    "text": "title: “Session 6 - Challenge - Company Segmentation” date: “7/24/2020” output: html_document: toc: TRUE theme: flatly highlight: tango code_folding: hide df_print: paged —"
  },
  {
    "objectID": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "href": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "title": "Machine Learninng Fundamentals",
    "section": "\n5.1 Step 1 - Convert stock prices to a standardized format (daily returns)",
    "text": "5.1 Step 1 - Convert stock prices to a standardized format (daily returns)\nWhat you first need to do is get the data in a format that can be converted to a “user-item” style matrix. The challenge here is to connect the dots between what we have and what we need to do to format it properly.\nWe know that in order to compare the data, it needs to be standardized or normalized. Why? Because we cannot compare values (stock prices) that are of completely different magnitudes. In order to standardize, we will convert from adjusted stock price (dollar value) to daily returns (percent change from previous day). Here is the formula.\n\\[\nreturn_{daily} = \\frac{price_{i}-price_{i-1}}{price_{i-1}}\n\\]\nFirst, what do we have? We have stock prices for every stock in the SP 500 Index, which is the daily stock prices for over 500 stocks. The data set is over 1.2M observations.\n\nsp_500_prices_tbl %>% glimpse()\n\n#> Rows: 1,225,765\n#> Columns: 8\n#> $ symbol   <chr> \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT…\n#> $ date     <date> 2009-01-02, 2009-01-05, 2009-01-06, 2009-01-07, 2009-01-08, …\n#> $ open     <dbl> 19.53, 20.20, 20.75, 20.19, 19.63, 20.17, 19.71, 19.52, 19.53…\n#> $ high     <dbl> 20.40, 20.67, 21.00, 20.29, 20.19, 20.30, 19.79, 19.99, 19.68…\n#> $ low      <dbl> 19.37, 20.06, 20.61, 19.48, 19.55, 19.41, 19.30, 19.52, 19.01…\n#> $ close    <dbl> 20.33, 20.52, 20.76, 19.51, 20.12, 19.52, 19.47, 19.82, 19.09…\n#> $ volume   <dbl> 50084000, 61475200, 58083400, 72709900, 70255400, 49815300, 5…\n#> $ adjusted <dbl> 15.86624, 16.01451, 16.20183, 15.22628, 15.70234, 15.23408, 1…\n\n\nYour first task is to convert to a tibble named sp_500_daily_returns_tbl by performing the following operations:\n\nSelect the symbol, date and adjusted columns\nFilter to dates beginning in the year 2018 and beyond.\nCompute a Lag of 1 day on the adjusted stock price. Be sure to group by symbol first, otherwise we will have lags computed using values from the previous stock in the data frame.\nRemove a NA values from the lagging operation\nCompute the difference between adjusted and the lag\nCompute the percentage difference by dividing the difference by that lag. Name this column pct_return.\nReturn only the symbol, date, and pct_return columns\nSave as a variable named sp_500_daily_returns_tbl\n\n\n\n# Apply your data transformation skills!\n\n#- Select the `symbol`, `date` and `adjusted` columns\n\nsp_500_daily_returns_tbl <- sp_500_prices_tbl %>% \n  select(symbol, date, adjusted)\n\n#- Filter to dates beginning in the year 2018 and beyond. \n\nsp_500_daily_returns_tbl <- subset(sp_500_daily_returns_tbl, format(date, \"%Y\") >= \"2018\")\n\n# - Compute a Lag of 1 day on the adjusted stock price. Be sure to group by symbol first, otherwise we will have lags computed using values from the previous stock in the data frame. \n# - Remove a `NA` values from the lagging operation\n\nsp_500_daily_returns_tbl <- sp_500_daily_returns_tbl %>%\n  group_by(symbol) %>%\n  mutate(lag_price = lag(adjusted)) %>%\n  na.omit(sp_500_daily_returns_tbl)\n\n\n# - Compute the difference between adjusted and the lag\n\nsp_500_daily_returns_tbl <- sp_500_daily_returns_tbl %>%\n  mutate(diff_price = adjusted - lag_price)\n\n# - Compute the percentage difference by dividing the difference by that lag. Name this column `pct_return`.\n\nsp_500_daily_returns_tbl <- sp_500_daily_returns_tbl %>%\n  mutate(pct_return = diff_price/ lag_price ) %>%\n  select(symbol, date, pct_return)\n\n\n# Output: sp_500_daily_returns_tbl\n\nsp_500_daily_returns_tbl"
  },
  {
    "objectID": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-2---convert-to-user-item-format",
    "href": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-2---convert-to-user-item-format",
    "title": "Machine Learninng Fundamentals",
    "section": "\n5.2 Step 2 - Convert to User-Item Format",
    "text": "5.2 Step 2 - Convert to User-Item Format\nThe next step is to convert to a user-item format with the symbol in the first column and every other column the value of the daily returns (pct_return) for every stock at each date.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nsp_500_daily_returns_tbl <- read_rds(\"sp_500_daily_returns_tbl.rds\")\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\n\nNow that we have the daily returns (percentage change from one day to the next), we can convert to a user-item format. The user in this case is the symbol (company), and the item in this case is the pct_return at each date.\n\nSpread the date column to get the values as percentage returns. Make sure to fill an NA values with zeros.\nSave the result as stock_date_matrix_tbl\n\n\n\n# Convert to User-Item Format\n\n# - Spread the `date` column to get the values as percentage returns. Make sure to fill an `NA` values with zeros. \n\nstock_date_matrix_tbl <- sp_500_daily_returns_tbl %>%\n  spread(date,pct_return, fill = 0)\nstock_date_matrix_tbl\n\n\n\n  \n\n\n# Output: stock_date_matrix_tbl"
  },
  {
    "objectID": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-3---perform-k-means-clustering",
    "href": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-3---perform-k-means-clustering",
    "title": "Machine Learninng Fundamentals",
    "section": "\n5.3 Step 3 - Perform K-Means Clustering",
    "text": "5.3 Step 3 - Perform K-Means Clustering\nNext, we’ll perform K-Means clustering.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nstock_date_matrix_tbl <- read_rds(\"stock_date_matrix_tbl.rds\")\nstock_date_matrix_tbl\n\n\n\n  \n\n\n\nBeginning with the stock_date_matrix_tbl, perform the following operations:\n\nDrop the non-numeric column, symbol\n\nPerform kmeans() with centers = 4 and nstart = 20\n\nSave the result as kmeans_obj\n\n\n\n# Create kmeans_obj for 4 centers\n\n# - Drop the non-numeric column, `symbol`\n\nstock_date_matrix_numeric_tbl <- stock_date_matrix_tbl %>%\n  select(-symbol)\n\nkmeans_obj <- stock_date_matrix_numeric_tbl %>%\n    kmeans(centers = 4, nstart = 20)\n\nkmeans_obj$cluster\n\n#>   [1] 2 1 2 1 2 2 1 2 2 1 1 2 2 2 1 3 3 3 2 2 2 3 2 2 1 2 1 2 2 2 1 1 1 2 2 2 2\n#>  [38] 3 1 1 1 2 2 2 4 4 2 2 2 3 2 3 1 3 1 2 3 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2\n#>  [75] 2 2 2 3 2 3 2 2 2 2 2 2 3 1 2 2 2 2 2 3 2 2 2 2 3 3 2 2 2 2 2 3 2 3 2 2 2\n#> [112] 4 2 2 3 2 2 1 1 2 2 2 2 2 2 4 4 3 2 2 2 2 2 2 2 2 2 2 2 3 2 2 3 2 3 3 2 4\n#> [149] 2 2 1 2 2 3 2 3 2 2 2 4 3 3 3 3 2 2 3 3 1 3 2 2 3 2 4 2 1 2 4 2 3 1 2 2 2\n#> [186] 2 2 4 2 2 2 2 2 2 3 4 1 2 2 2 2 3 2 2 1 1 2 1 2 2 2 2 2 4 2 2 2 2 3 2 4 4\n#> [223] 2 2 2 2 2 2 4 1 2 2 3 2 2 2 3 2 2 2 1 2 1 1 2 1 1 2 2 1 2 2 3 1 2 2 2 2 2\n#> [260] 2 2 2 2 2 2 2 3 2 1 3 3 1 3 4 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 1 2 3 2\n#> [297] 2 1 3 3 2 2 1 3 1 2 2 3 2 2 1 2 3 2 2 2 2 3 2 4 2 4 2 1 1 2 2 2 1 1 2 4 2\n#> [334] 2 3 2 1 3 2 1 2 2 4 3 2 1 2 2 1 2 2 2 3 4 2 2 2 4 2 2 2 3 3 2 2 3 2 2 2 2\n#> [371] 2 3 3 2 2 3 2 3 2 2 3 4 2 2 4 1 2 1 2 2 3 2 2 1 2 2 2 2 2 2 2 2 2 2 3 2 2\n#> [408] 2 2 2 3 4 3 2 1 3 3 2 3 2 2 1 2 2 1 2 2 1 3 2 3 2 2 2 2 2 2 2 2 2 1 2 2 2\n#> [445] 3 1 1 1 1 2 1 1 2 3 2 2 2 2 2 2 1 2 2 1 2 2 2 4 2 3 2 1 1 3 3 2 2 2 1 3 3\n#> [482] 2 2 2 3 4 2 2 2 3 1 4 3 1 4 2 2 2 2 2 2 2\n\n\nUse glance() to get the tot.withinss.\n\n# Apply glance() to get the tot.withinss\n\nbroom::glance(kmeans_obj)"
  },
  {
    "objectID": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-4---find-the-optimal-value-of-k",
    "href": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-4---find-the-optimal-value-of-k",
    "title": "Machine Learninng Fundamentals",
    "section": "\n5.4 Step 4 - Find the optimal value of K",
    "text": "5.4 Step 4 - Find the optimal value of K\nNow that we are familiar with the process for calculating kmeans(), let’s use purrr to iterate over many values of “k” using the centers argument.\nWe’ll use this custom function called kmeans_mapper():\n\nkmeans_mapper <- function(center = 3) {\n    stock_date_matrix_tbl %>%\n        select(-symbol) %>%\n        kmeans(centers = center, nstart = 20)\n}\n\nApply the kmeans_mapper() and glance() functions iteratively using purrr.\n\nCreate a tibble containing column called centers that go from 1 to 30\nAdd a column named k_means with the kmeans_mapper() output. Use mutate() to add the column and map() to map centers to the kmeans_mapper() function.\nAdd a column named glance with the glance() output. Use mutate() and map() again to iterate over the column of k_means.\nSave the output as k_means_mapped_tbl\n\n\n\n# Use purrr to map\n\nkmeans_mapped_tbl <- tibble(centers = 1:15) %>%\n    mutate(k_means = centers %>% map(kmeans_mapper)) %>%\n    mutate(glance  = k_means %>% map(glance))\n\n# Output: k_means_mapped_tbl \n\nNext, let’s visualize the “tot.withinss” from the glance output as a Scree Plot.\n\nBegin with the k_means_mapped_tbl\n\nUnnest the glance column\nPlot the centers column (x-axis) versus the tot.withinss column (y-axis) using geom_point() and geom_line()\n\nAdd a title “Scree Plot” and feel free to style it with your favorite theme\n\n\n# Visualize Scree Plot\n\n# - Begin with the `k_means_mapped_tbl`\n# - Unnest the `glance` column\n\nkmeans_mapped_tbl %>%\n    unnest(glance) %>%\n    select(centers, tot.withinss) %>%\n  # Visualization\n    ggplot(aes(centers, tot.withinss)) +\n    geom_point(color = \"#2DC6D6\", size = 4) +\n    geom_line(color = \"#2DC6D6\", size = 1) +\n    # Add labels (which are repelled a little)\n    ggrepel::geom_label_repel(aes(label = centers), color = \"#2DC6D6\") + \n    \n    # Formatting\n    labs(title = \"Skree Plot\",\n    caption = \"We can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K.\")\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nWe can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K."
  },
  {
    "objectID": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-5---apply-umap",
    "href": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-5---apply-umap",
    "title": "Machine Learninng Fundamentals",
    "section": "\n5.5 Step 5 - Apply UMAP",
    "text": "5.5 Step 5 - Apply UMAP\nNext, let’s plot the UMAP 2D visualization to help us investigate cluster assignments.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nk_means_mapped_tbl <- read_rds(\"k_means_mapped_tbl.rds\")\n\nFirst, let’s apply the umap() function to the stock_date_matrix_tbl, which contains our user-item matrix in tibble format.\n\nStart with stock_date_matrix_tbl\n\nDe-select the symbol column\nUse the umap() function storing the output as umap_results\n\n\n\n# Apply UMAP\n\numap_results <- stock_date_matrix_tbl %>%\n    select(-symbol) %>%\n    umap()\n\n# Store results as: umap_results \n\nNext, we want to combine the layout from the umap_results with the symbol column from the stock_date_matrix_tbl.\n\nStart with umap_results$layout\n\nConvert from a matrix data type to a tibble with as_tibble()\n\nBind the columns of the umap tibble with the symbol column from the stock_date_matrix_tbl.\nSave the results as umap_results_tbl.\n\n\n# Convert umap results to tibble with symbols\n\numap_results_tbl <- umap_results$layout %>%\n    as_tibble(.name_repair = \"unique\") %>% # argument is required to set names in the next step\n    set_names(c(\"x\", \"y\")) %>%\n    bind_cols(\n        stock_date_matrix_tbl %>% select(symbol)\n    )\n\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n\n# Output: umap_results_tbl\n\nFinally, let’s make a quick visualization of the umap_results_tbl.\n\nPipe the umap_results_tbl into ggplot() mapping the columns to x-axis and y-axis\nAdd a geom_point() geometry with an alpha = 0.5\n\nApply theme_tq() and add a title “UMAP Projection”\n\n\n# Visualize UMAP results\n\numap_results_tbl %>%\n    ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    theme_tq() +\n    labs(title = \"UMAP Projection\")\n\n\n\n\n\n\numap_results_tbl\n\n\n\n  \n\n\n\nWe can now see that we have some clusters. However, we still need to combine the K-Means clusters and the UMAP 2D representation."
  },
  {
    "objectID": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-6---combine-k-means-and-umap",
    "href": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#step-6---combine-k-means-and-umap",
    "title": "Machine Learninng Fundamentals",
    "section": "\n5.6 Step 6 - Combine K-Means and UMAP",
    "text": "5.6 Step 6 - Combine K-Means and UMAP\nNext, we combine the K-Means clusters and the UMAP 2D representation\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nk_means_mapped_tbl <- read_rds(\"k_means_mapped_tbl.rds\")\numap_results_tbl   <- read_rds(\"umap_results_tbl.rds\")\n\nFirst, pull out the K-Means for 10 Centers. Use this since beyond this value the Scree Plot flattens. Have a look at the business case to recall how that works.\n\n# Get the k_means_obj from the 10th center\n\nk_means_obj <- kmeans_mapped_tbl %>%\n    pull(k_means) %>%\n    pluck(10)\n\n# Store as k_means_obj\n\nNext, we’ll combine the clusters from the k_means_obj with the umap_results_tbl.\n\nBegin with the k_means_obj\n\nAugment the k_means_obj with the stock_date_matrix_tbl to get the clusters added to the end of the tibble\nSelect just the symbol and .cluster columns\nLeft join the result with the umap_results_tbl by the symbol column\nLeft join the result with the result of sp_500_index_tbl %>% select(symbol, company, sector) by the symbol column.\nStore the output as umap_kmeans_results_tbl\n\n\n\n# Use your dplyr & broom skills to combine the k_means_obj with the umap_results_tbl\n\n# Convert it to a tibble with broom\nkmeans_10_clusters_tbl <- k_means_obj %>% \n    augment(stock_date_matrix_tbl) %>%\n    # Select the data we need\n    select(symbol, .cluster)\n\n# Bind data together\njoined_data <- umap_results_tbl %>%\n    left_join(kmeans_10_clusters_tbl, by = \"symbol\")\n\numap_kmeans_results_tbl <- joined_data %>%\n  left_join(sp_500_index_tbl %>% select(symbol, company, sector), by = \"symbol\" )\n\numap_kmeans_results_tbl\n\n\n\n  \n\n\n# Output: umap_kmeans_results_tbl \n\nPlot the K-Means and UMAP results.\n\nBegin with the umap_kmeans_results_tbl\n\nUse ggplot() mapping V1, V2 and color = .cluster\n\nAdd the geom_point() geometry with alpha = 0.5\n\nApply colors as you desire (e.g. scale_color_manual(values = palette_light() %>% rep(3)))\n\n\n# Visualize the combined K-Means and UMAP results\n\numap_kmeans_results_tbl %>%\n    mutate(label_text = str_glue(\"CompanyStockPrice: {symbol}\n                                 Cluster: {.cluster}\")) %>%\n    \n    ggplot(aes(V1, V2, color = .cluster)) +\n    \n    # Geometries\n    geom_point(alpta = 0.5) +\n    \n    # Formatting\n    scale_color_manual(values=c(\"#2d72d6\", \"#2dc6d6\", \"#2dd692\", \"#af9244\", \"#9d61c7\", \"#a5e461\", \"#a1306c\", \"#096f40\", \"#8d0a0c\", \"#e3792d\" )) +\n    labs(title = \"K Clustering for Stock Price Analysis\")+\n    theme(legend.position = \"none\")\n\n#> Warning in geom_point(alpta = 0.5): Ignoring unknown parameters: `alpta`"
  },
  {
    "objectID": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#congratulations-you-are-done-with-the-1st-challenge",
    "href": "content/01_journal/Challenge1/05_Machine_Learning_Fundamentals.html#congratulations-you-are-done-with-the-1st-challenge",
    "title": "Machine Learninng Fundamentals",
    "section": "\n5.7 Congratulations! You are done with the 1st challenge!",
    "text": "5.7 Congratulations! You are done with the 1st challenge!"
  },
  {
    "objectID": "content/01_journal/Challenge1/Chapter_1_Challenge.html",
    "href": "content/01_journal/Challenge1/Chapter_1_Challenge.html",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "",
    "text": "Your organization wants to know which companies are similar to each other to help in identifying potential customers of a SAAS software solution (e.g. Salesforce CRM or equivalent) in various segments of the market. The Sales Department is very interested in this analysis, which will help them more easily penetrate various market segments.\nYou will be using stock prices in this analysis. You come up with a method to classify companies based on how their stocks trade using their daily stock returns (percentage movement from one day to the next). This analysis will help your organization determine which companies are related to each other (competitors and have similar attributes).\nYou can analyze the stock prices using what you’ve learned in the unsupervised learning tools including K-Means and UMAP. You will use a combination of kmeans() to find groups and umap() to visualize similarity of daily stock returns."
  },
  {
    "objectID": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "href": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.1 Step 1 - Convert stock prices to a standardized format (daily returns)",
    "text": "5.1 Step 1 - Convert stock prices to a standardized format (daily returns)\nWhat you first need to do is get the data in a format that can be converted to a “user-item” style matrix. The challenge here is to connect the dots between what we have and what we need to do to format it properly.\nWe know that in order to compare the data, it needs to be standardized or normalized. Why? Because we cannot compare values (stock prices) that are of completely different magnitudes. In order to standardize, we will convert from adjusted stock price (dollar value) to daily returns (percent change from previous day). Here is the formula.\n\\[\nreturn_{daily} = \\frac{price_{i}-price_{i-1}}{price_{i-1}}\n\\]\nFirst, what do we have? We have stock prices for every stock in the SP 500 Index, which is the daily stock prices for over 500 stocks. The data set is over 1.2M observations.\n\nsp_500_prices_tbl %>% glimpse()\n\n#> Rows: 1,225,765\n#> Columns: 8\n#> $ symbol   <chr> \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT…\n#> $ date     <date> 2009-01-02, 2009-01-05, 2009-01-06, 2009-01-07, 2009-01-08, …\n#> $ open     <dbl> 19.53, 20.20, 20.75, 20.19, 19.63, 20.17, 19.71, 19.52, 19.53…\n#> $ high     <dbl> 20.40, 20.67, 21.00, 20.29, 20.19, 20.30, 19.79, 19.99, 19.68…\n#> $ low      <dbl> 19.37, 20.06, 20.61, 19.48, 19.55, 19.41, 19.30, 19.52, 19.01…\n#> $ close    <dbl> 20.33, 20.52, 20.76, 19.51, 20.12, 19.52, 19.47, 19.82, 19.09…\n#> $ volume   <dbl> 50084000, 61475200, 58083400, 72709900, 70255400, 49815300, 5…\n#> $ adjusted <dbl> 15.86624, 16.01451, 16.20183, 15.22628, 15.70234, 15.23408, 1…\n\n\nYour first task is to convert to a tibble named sp_500_daily_returns_tbl by performing the following operations:\n\nSelect the symbol, date and adjusted columns\nFilter to dates beginning in the year 2018 and beyond.\nCompute a Lag of 1 day on the adjusted stock price. Be sure to group by symbol first, otherwise we will have lags computed using values from the previous stock in the data frame.\nRemove a NA values from the lagging operation\nCompute the difference between adjusted and the lag\nCompute the percentage difference by dividing the difference by that lag. Name this column pct_return.\nReturn only the symbol, date, and pct_return columns\nSave as a variable named sp_500_daily_returns_tbl\n\n\n\n# Apply your data transformation skills!\n\n#- Select the `symbol`, `date` and `adjusted` columns\n\nsp_500_daily_returns_tbl <- sp_500_prices_tbl %>% \n  select(symbol, date, adjusted)\n\n#- Filter to dates beginning in the year 2018 and beyond. \n\nsp_500_daily_returns_tbl <- subset(sp_500_daily_returns_tbl, format(date, \"%Y\") >= \"2018\")\n\n# - Compute a Lag of 1 day on the adjusted stock price. Be sure to group by symbol first, otherwise we will have lags computed using values from the previous stock in the data frame. \n# - Remove a `NA` values from the lagging operation\n\nsp_500_daily_returns_tbl <- sp_500_daily_returns_tbl %>%\n  group_by(symbol) %>%\n  mutate(lag_price = lag(adjusted)) %>%\n  na.omit(sp_500_daily_returns_tbl)\n\n\n# - Compute the difference between adjusted and the lag\n\nsp_500_daily_returns_tbl <- sp_500_daily_returns_tbl %>%\n  mutate(diff_price = adjusted - lag_price)\n\n# - Compute the percentage difference by dividing the difference by that lag. Name this column `pct_return`.\n\nsp_500_daily_returns_tbl <- sp_500_daily_returns_tbl %>%\n  mutate(pct_return = diff_price/ lag_price ) %>%\n  select(symbol, date, pct_return)\n\n\n# Output: sp_500_daily_returns_tbl\n\nsp_500_daily_returns_tbl"
  },
  {
    "objectID": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-2---convert-to-user-item-format",
    "href": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-2---convert-to-user-item-format",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.2 Step 2 - Convert to User-Item Format",
    "text": "5.2 Step 2 - Convert to User-Item Format\nThe next step is to convert to a user-item format with the symbol in the first column and every other column the value of the daily returns (pct_return) for every stock at each date.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nsp_500_daily_returns_tbl <- read_rds(\"sp_500_daily_returns_tbl.rds\")\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\n\nNow that we have the daily returns (percentage change from one day to the next), we can convert to a user-item format. The user in this case is the symbol (company), and the item in this case is the pct_return at each date.\n\nSpread the date column to get the values as percentage returns. Make sure to fill an NA values with zeros.\nSave the result as stock_date_matrix_tbl\n\n\n\n# Convert to User-Item Format\n\n# - Spread the `date` column to get the values as percentage returns. Make sure to fill an `NA` values with zeros. \n\nstock_date_matrix_tbl <- sp_500_daily_returns_tbl %>%\n  spread(date,pct_return, fill = 0)\nstock_date_matrix_tbl\n\n\n\n  \n\n\n# Output: stock_date_matrix_tbl"
  },
  {
    "objectID": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-3---perform-k-means-clustering",
    "href": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-3---perform-k-means-clustering",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.3 Step 3 - Perform K-Means Clustering",
    "text": "5.3 Step 3 - Perform K-Means Clustering\nNext, we’ll perform K-Means clustering.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nstock_date_matrix_tbl <- read_rds(\"stock_date_matrix_tbl.rds\")\nstock_date_matrix_tbl\n\n\n\n  \n\n\n\nBeginning with the stock_date_matrix_tbl, perform the following operations:\n\nDrop the non-numeric column, symbol\n\nPerform kmeans() with centers = 4 and nstart = 20\n\nSave the result as kmeans_obj\n\n\n\n# Create kmeans_obj for 4 centers\n\n# - Drop the non-numeric column, `symbol`\n\nstock_date_matrix_numeric_tbl <- stock_date_matrix_tbl %>%\n  select(-symbol)\n\nkmeans_obj <- stock_date_matrix_numeric_tbl %>%\n    kmeans(centers = 4, nstart = 20)\n\nkmeans_obj$cluster\n\n#>   [1] 2 4 2 4 2 2 4 2 2 4 4 2 2 2 4 1 1 1 2 2 2 1 2 2 4 2 4 2 2 2 4 4 4 2 2 2 2\n#>  [38] 1 4 4 4 2 2 2 3 3 2 2 2 1 2 1 4 1 4 2 1 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2\n#>  [75] 2 2 2 1 2 1 2 2 2 2 2 2 1 4 2 2 2 2 2 1 2 2 2 2 1 1 2 2 2 2 2 1 2 1 2 2 2\n#> [112] 3 2 2 1 2 2 4 4 2 2 2 2 2 2 3 3 1 2 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 1 1 2 3\n#> [149] 2 2 4 2 2 1 2 1 2 2 2 3 1 1 1 1 2 2 1 1 4 1 2 2 1 2 3 2 4 2 3 2 1 4 2 2 2\n#> [186] 2 2 3 2 2 2 2 2 2 1 3 4 2 2 2 2 1 2 2 4 4 2 4 2 2 2 2 2 3 2 2 2 2 1 2 3 3\n#> [223] 2 2 2 2 2 2 3 4 2 2 1 2 2 2 1 2 2 2 4 2 4 4 2 4 4 2 2 4 2 2 1 4 2 2 2 2 2\n#> [260] 2 2 2 2 2 2 2 1 2 4 1 1 4 1 3 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 4 2 1 2\n#> [297] 2 4 1 1 2 2 4 1 4 2 2 1 2 2 4 2 1 2 2 2 2 1 2 3 2 3 2 4 4 2 2 2 4 4 2 3 2\n#> [334] 2 1 2 4 1 2 4 2 2 3 1 2 4 2 2 4 2 2 2 1 3 2 2 2 3 2 2 2 1 1 2 2 1 2 2 2 2\n#> [371] 2 1 1 2 2 1 2 1 2 2 1 3 2 2 3 4 2 4 2 2 1 2 2 4 2 2 2 2 2 2 2 2 2 2 1 2 2\n#> [408] 2 2 2 1 3 1 2 4 1 1 2 1 2 2 4 2 2 4 2 2 4 1 2 1 2 2 2 2 2 2 2 2 2 4 2 2 2\n#> [445] 1 4 4 4 4 2 4 4 2 1 2 2 2 2 2 2 4 2 2 4 2 2 2 3 2 1 2 4 4 1 1 2 2 2 4 1 1\n#> [482] 2 2 2 1 3 2 2 2 1 4 3 1 4 3 2 2 2 2 2 2 2\n\n\nUse glance() to get the tot.withinss.\n\n# Apply glance() to get the tot.withinss\n\nbroom::glance(kmeans_obj)"
  },
  {
    "objectID": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-4---find-the-optimal-value-of-k",
    "href": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-4---find-the-optimal-value-of-k",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.4 Step 4 - Find the optimal value of K",
    "text": "5.4 Step 4 - Find the optimal value of K\nNow that we are familiar with the process for calculating kmeans(), let’s use purrr to iterate over many values of “k” using the centers argument.\nWe’ll use this custom function called kmeans_mapper():\n\nkmeans_mapper <- function(center = 3) {\n    stock_date_matrix_tbl %>%\n        select(-symbol) %>%\n        kmeans(centers = center, nstart = 20)\n}\n\nApply the kmeans_mapper() and glance() functions iteratively using purrr.\n\nCreate a tibble containing column called centers that go from 1 to 30\nAdd a column named k_means with the kmeans_mapper() output. Use mutate() to add the column and map() to map centers to the kmeans_mapper() function.\nAdd a column named glance with the glance() output. Use mutate() and map() again to iterate over the column of k_means.\nSave the output as k_means_mapped_tbl\n\n\n\n# Use purrr to map\n\nkmeans_mapped_tbl <- tibble(centers = 1:15) %>%\n    mutate(k_means = centers %>% map(kmeans_mapper)) %>%\n    mutate(glance  = k_means %>% map(glance))\n\n# Output: k_means_mapped_tbl \n\nNext, let’s visualize the “tot.withinss” from the glance output as a Scree Plot.\n\nBegin with the k_means_mapped_tbl\n\nUnnest the glance column\nPlot the centers column (x-axis) versus the tot.withinss column (y-axis) using geom_point() and geom_line()\n\nAdd a title “Scree Plot” and feel free to style it with your favorite theme\n\n\n# Visualize Scree Plot\n\n# - Begin with the `k_means_mapped_tbl`\n# - Unnest the `glance` column\n\nkmeans_mapped_tbl %>%\n    unnest(glance) %>%\n    select(centers, tot.withinss) %>%\n  # Visualization\n    ggplot(aes(centers, tot.withinss)) +\n    geom_point(color = \"#2DC6D6\", size = 4) +\n    geom_line(color = \"#2DC6D6\", size = 1) +\n    # Add labels (which are repelled a little)\n    ggrepel::geom_label_repel(aes(label = centers), color = \"#2DC6D6\") + \n    \n    # Formatting\n    labs(title = \"Skree Plot\",\n    caption = \"We can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K.\")\n\n\n\n\n\n\n\nWe can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K."
  },
  {
    "objectID": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-5---apply-umap",
    "href": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-5---apply-umap",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.5 Step 5 - Apply UMAP",
    "text": "5.5 Step 5 - Apply UMAP\nNext, let’s plot the UMAP 2D visualization to help us investigate cluster assignments.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nk_means_mapped_tbl <- read_rds(\"k_means_mapped_tbl.rds\")\n\nFirst, let’s apply the umap() function to the stock_date_matrix_tbl, which contains our user-item matrix in tibble format.\n\nStart with stock_date_matrix_tbl\n\nDe-select the symbol column\nUse the umap() function storing the output as umap_results\n\n\n\n# Apply UMAP\n\numap_results <- stock_date_matrix_tbl %>%\n    select(-symbol) %>%\n    umap()\n\n# Store results as: umap_results \n\nNext, we want to combine the layout from the umap_results with the symbol column from the stock_date_matrix_tbl.\n\nStart with umap_results$layout\n\nConvert from a matrix data type to a tibble with as_tibble()\n\nBind the columns of the umap tibble with the symbol column from the stock_date_matrix_tbl.\nSave the results as umap_results_tbl.\n\n\n# Convert umap results to tibble with symbols\n\numap_results_tbl <- umap_results$layout %>%\n    as_tibble(.name_repair = \"unique\") %>% # argument is required to set names in the next step\n    set_names(c(\"x\", \"y\")) %>%\n    bind_cols(\n        stock_date_matrix_tbl %>% select(symbol)\n    )\n\n# Output: umap_results_tbl\n\nFinally, let’s make a quick visualization of the umap_results_tbl.\n\nPipe the umap_results_tbl into ggplot() mapping the columns to x-axis and y-axis\nAdd a geom_point() geometry with an alpha = 0.5\n\nApply theme_tq() and add a title “UMAP Projection”\n\n\n# Visualize UMAP results\n\numap_results_tbl %>%\n    ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    theme_tq() +\n    labs(title = \"UMAP Projection\")\n\n\n\n\n\n\numap_results_tbl\n\n\n\n  \n\n\n\nWe can now see that we have some clusters. However, we still need to combine the K-Means clusters and the UMAP 2D representation."
  },
  {
    "objectID": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-6---combine-k-means-and-umap",
    "href": "content/01_journal/Challenge1/Chapter_1_Challenge.html#step-6---combine-k-means-and-umap",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.6 Step 6 - Combine K-Means and UMAP",
    "text": "5.6 Step 6 - Combine K-Means and UMAP\nNext, we combine the K-Means clusters and the UMAP 2D representation\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nk_means_mapped_tbl <- read_rds(\"k_means_mapped_tbl.rds\")\numap_results_tbl   <- read_rds(\"umap_results_tbl.rds\")\n\nFirst, pull out the K-Means for 10 Centers. Use this since beyond this value the Scree Plot flattens. Have a look at the business case to recall how that works.\n\n# Get the k_means_obj from the 10th center\n\nk_means_obj <- kmeans_mapped_tbl %>%\n    pull(k_means) %>%\n    pluck(10)\n\n# Store as k_means_obj\n\nNext, we’ll combine the clusters from the k_means_obj with the umap_results_tbl.\n\nBegin with the k_means_obj\n\nAugment the k_means_obj with the stock_date_matrix_tbl to get the clusters added to the end of the tibble\nSelect just the symbol and .cluster columns\nLeft join the result with the umap_results_tbl by the symbol column\nLeft join the result with the result of sp_500_index_tbl %>% select(symbol, company, sector) by the symbol column.\nStore the output as umap_kmeans_results_tbl\n\n\n\n# Use your dplyr & broom skills to combine the k_means_obj with the umap_results_tbl\n\n# Convert it to a tibble with broom\nkmeans_10_clusters_tbl <- k_means_obj %>% \n    augment(stock_date_matrix_tbl) %>%\n    # Select the data we need\n    select(symbol, .cluster)\n\n# Bind data together\njoined_data <- umap_results_tbl %>%\n    left_join(kmeans_10_clusters_tbl, by = \"symbol\")\n\numap_kmeans_results_tbl <- joined_data %>%\n  left_join(sp_500_index_tbl %>% select(symbol, company, sector), by = \"symbol\" )\n\numap_kmeans_results_tbl\n\n\n\n  \n\n\n# Output: umap_kmeans_results_tbl \n\nPlot the K-Means and UMAP results.\n\nBegin with the umap_kmeans_results_tbl\n\nUse ggplot() mapping V1, V2 and color = .cluster\n\nAdd the geom_point() geometry with alpha = 0.5\n\nApply colors as you desire (e.g. scale_color_manual(values = palette_light() %>% rep(3)))\n\n\n# Visualize the combined K-Means and UMAP results\n\numap_kmeans_results_tbl %>%\n    mutate(label_text = str_glue(\"CompanyStockPrice: {symbol}\n                                 Cluster: {.cluster}\")) %>%\n    \n    ggplot(aes(V1, V2, color = .cluster)) +\n    \n    # Geometries\n    geom_point(alpta = 0.5) +\n    \n    # Formatting\n    scale_color_manual(values=c(\"#2d72d6\", \"#2dc6d6\", \"#2dd692\", \"#af9244\", \"#9d61c7\", \"#a5e461\", \"#a1306c\", \"#096f40\", \"#8d0a0c\", \"#e3792d\" )) +\n    labs(title = \"K Clustering for Stock Price Analysis\")+\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\nCongratulations! You are done with the 1st challenge!"
  },
  {
    "objectID": "content/01_journal/Challenge2/Chapter_2_Challenge.html",
    "href": "content/01_journal/Challenge2/Chapter_2_Challenge.html",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "",
    "text": "1 Libraries\nLoad the following libraries.\n\n# Standard\nlibrary(tidyverse)\n\n# Modeling\nlibrary(parsnip)\n\n# Preprocessing & Sampling\nlibrary(recipes)\nlibrary(rsample)\n\n# Modeling Error Metrics\nlibrary(yardstick)\n\n# Plotting Decision Trees\nlibrary(rpart.plot)\n\nlibrary(forcats)\n\nlibrary(workflows)\n\n\n2 Modeling —————————————————————-\n\nbike_orderlines_tbl <- readRDS(\"bike_orderlines.rds\")\nglimpse(bike_orderlines_tbl)\n\n#> Rows: 15,644\n#> Columns: 18\n#> $ order_id       <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7…\n#> $ order_line     <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1…\n#> $ order_date     <dttm> 2015-01-07, 2015-01-07, 2015-01-10, 2015-01-10, 2015-0…\n#> $ model          <chr> \"Spectral CF 7 WMN\", \"Ultimate CF SLX Disc 8.0 ETAP\", \"…\n#> $ model_year     <dbl> 2021, 2020, 2021, 2019, 2020, 2020, 2020, 2021, 2020, 2…\n#> $ category_1     <chr> \"Mountain\", \"Road\", \"Mountain\", \"Road\", \"Mountain\", \"Hy…\n#> $ category_2     <chr> \"Trail\", \"Race\", \"Trail\", \"Triathlon Bike\", \"Dirt Jump\"…\n#> $ category_3     <chr> \"Spectral\", \"Ultimate\", \"Neuron\", \"Speedmax\", \"Stitched…\n#> $ price          <dbl> 3119, 5359, 2729, 1749, 1219, 1359, 2529, 1559, 3899, 6…\n#> $ quantity       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1…\n#> $ total_price    <dbl> 3119, 5359, 2729, 1749, 1219, 1359, 2529, 1559, 3899, 6…\n#> $ frame_material <chr> \"carbon\", \"carbon\", \"carbon\", \"carbon\", \"aluminium\", \"c…\n#> $ weight         <dbl> 13.80, 7.44, 14.06, 8.80, 11.50, 8.80, 8.20, 8.85, 14.4…\n#> $ url            <chr> \"https://www.canyon.com/en-de/mountain-bikes/trail-bike…\n#> $ bikeshop       <chr> \"AlexandeRad\", \"AlexandeRad\", \"WITT-RAD\", \"WITT-RAD\", \"…\n#> $ location       <chr> \"Hamburg, Hamburg\", \"Hamburg, Hamburg\", \"Bremen, Bremen…\n#> $ lat            <dbl> 53.57532, 53.57532, 53.07379, 53.07379, 48.78234, 48.78…\n#> $ lng            <dbl> 10.015340, 10.015340, 8.826754, 8.826754, 9.180819, 9.1…\n\nmodel_sales_tbl <- bike_orderlines_tbl %>%\n    select(total_price, model, category_2, frame_material) %>%\n    \n    group_by(model, category_2, frame_material) %>%\n    summarise(total_sales = sum(total_price)) %>%\n    ungroup() %>%\n    \n    arrange(desc(total_sales))\n\n#> `summarise()` has grouped output by 'model', 'category_2'. You can override\n#> using the `.groups` argument.\n\nmodel_sales_tbl %>%\n    mutate(category_2 = as_factor(category_2) %>% \n               fct_reorder(total_sales, .fun = max) %>% \n               fct_rev()) %>%\n    \n    ggplot(aes(frame_material, total_sales)) +\n    geom_violin() +\n    geom_jitter(width = 0.1, alpha = 0.5, color = \"#2c3e50\") +\n    #coord_flip() +\n    facet_wrap(~ category_2) +\n    scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = \"M\", accuracy = 0.1)) +\n    tidyquant::theme_tq() +\n    labs(\n        title = \"Total Sales for Each Model\",\n        x = \"Frame Material\", y = \"Revenue\"\n    )\n\n#> Registered S3 method overwritten by 'quantmod':\n#>   method            from\n#>   as.zoo.data.frame zoo\n\n\n#> Warning: Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n\n\n#> Warning in max(data$density): kein nicht-fehlendes Argument für max; gebe -Inf\n#> zurück\n\n\n#> Warning: Computation failed in `stat_ydensity()`\n#> Caused by error in `$<-.data.frame`:\n#> ! Ersetzung hat 1 Zeile, Daten haben 0\n\n\n\n\n\n\n\n\n\n3 Features\n\nbike_features_tbl <- readRDS(\"bike_features_tbl.rds\")\nglimpse(bike_features_tbl)\n\n#> Rows: 231\n#> Columns: 67\n#> $ bike_id                     <dbl> 2875, 2873, 2874, 2876, 2877, 2225, 2091, …\n#> $ model                       <chr> \"Aeroad CF SL Disc 8.0 Di2\", \"Aeroad CF SL…\n#> $ model_year                  <dbl> 2020, 2020, 2020, 2020, 2020, 2019, 2019, …\n#> $ frame_material              <chr> \"carbon\", \"carbon\", \"carbon\", \"carbon\", \"c…\n#> $ weight                      <dbl> 7.60, 7.27, 7.10, 7.73, 7.83, 6.80, 6.80, …\n#> $ price                       <dbl> 4579, 6919, 6429, 5069, 3609, 6139, 5359, …\n#> $ category_1                  <chr> \"Road\", \"Road\", \"Road\", \"Road\", \"Road\", \"R…\n#> $ category_2                  <chr> \"Race\", \"Race\", \"Race\", \"Race\", \"Race\", \"R…\n#> $ category_3                  <chr> \"Aeroad\", \"Aeroad\", \"Aeroad\", \"Aeroad\", \"A…\n#> $ gender                      <chr> \"unisex\", \"unisex\", \"unisex\", \"unisex\", \"u…\n#> $ url                         <chr> \"https://www.canyon.com/en-de/road-bikes/r…\n#> $ Frame                       <chr> \"Canyon Aeroad CF SL Disc\", \"Canyon Aeroad…\n#> $ Fork                        <chr> \"Canyon FK0041 CF SLX Disc\", \"Canyon FK004…\n#> $ `Rear Derailleur`           <chr> \"Shimano Ultegra Di2 R8050 SS\", \"SRAM RED …\n#> $ `Front Derailleur`          <chr> \"Shimano Ultegra Di2 R8050\", \"SRAM RED eTa…\n#> $ Cassette                    <chr> \"Shimano Ultegra R8000, 11-speed, 11-28T\",…\n#> $ Crank                       <chr> \"Shimano Ultegra R8000\", \"SRAM RED D1\", \"S…\n#> $ `Bottom bracket`            <chr> \"Shimano Pressfit BB72\", \"SRAM Pressfit RE…\n#> $ `Thru Axle`                 <chr> \"Canyon Thru Axle\", \"Canyon Thru Axle\", \"C…\n#> $ Cockpit                     <chr> \"Canyon H36 Aerocockpit CF\", \"Canyon H36 A…\n#> $ Saddle                      <chr> \"Selle Italia SLR\", \"Selle Italia SLR\", \"S…\n#> $ Seatpost                    <chr> \"Canyon S27 Aero VCLS CF\", \"Canyon S27 Aer…\n#> $ Pedals                      <chr> \"None included\", \"None included\", \"None in…\n#> $ `Derailleur hanger`         <chr> \"Shop Derailleur Hanger GP0211-01\", \"Shop …\n#> $ Battery                     <chr> \"\", \"SRAM eTap Powerpack\", \"\", \"SRAM eTap …\n#> $ Brake                       <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Shift Lever`               <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"Shimano Di2 Remot…\n#> $ Chain                       <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"Shimano CN-HG901 …\n#> $ Stem                        <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Canyon V13\", …\n#> $ Handlebar                   <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Canyon H16 Ae…\n#> $ Headset                     <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Motor                       <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Battery Charger`           <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Flat Pedals`               <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Chainguard                  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Aero Bar`                  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Brake Lever / Master`      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Wheel Tire System`         <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Suspension Fork`           <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Disc Brake`                <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Grips                       <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Chainring                   <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Display                     <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Modeswitch                  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Rear Shock`                <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Light                       <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Fender                      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Bike Racks`                <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Brake 1`                   <chr> \"\", \"\", \"\", \"\", \"\", \"SRAM S-900 Direct Mou…\n#> $ `Brake 2`                   <chr> \"\", \"\", \"\", \"\", \"\", \"SRAM S-900 Direct Mou…\n#> $ `Shift-/ Brake Lever 1`     <chr> \"Shimano Ultegra Di2 R8070, 11-speed\", \"SR…\n#> $ `Shift-/ Brake Lever 2`     <chr> \"Shimano Ultegra Di2 R8070, 11-speed\", \"SR…\n#> $ `Wheel 1`                   <chr> \"DT Swiss ARC 1400 Dicut\", \"DT Swiss ARC 1…\n#> $ `Wheel 2`                   <chr> \"DT Swiss ARC 1400 Dicut\", \"DT Swiss ARC 1…\n#> $ `Tyre 1`                    <chr> \"Continental Grand Prix 5000 / Attack  23 …\n#> $ `Tyre 2`                    <chr> \"Continental Grand Prix 5000, 25 mm\", \"Con…\n#> $ `Handlebar Tape 1`          <chr> \"Canyon Ergospeed Gel\", \"Canyon Ergospeed …\n#> $ `Handlebar Tape 2`          <chr> \"Canyon bar-end plug\", \"Canyon bar-end plu…\n#> $ `Manuals and Accessories 1` <chr> \"Canyon tool case\", \"Canyon tool case\", \"C…\n#> $ `Manuals and Accessories 2` <chr> \"DT Swiss warranty & intended use manual\",…\n#> $ `Manuals and Accessories 3` <chr> \"Canyon starter box\", \"Canyon starter box\"…\n#> $ `Manuals and Accessories 4` <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"BAG R…\n#> $ `Manuals and Accessories 5` <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Manuals and Accessories 6` <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Manuals and Accessories 7` <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Manuals and Accessories 8` <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ `Brake Rotor`               <list> \"Shimano RT800\", \"SRAM Centerline X\", \"Sh…\n\nbike_features_tbl <- bike_features_tbl %>% \n    select(model:url, `Rear Derailleur`, `Shift Lever`) %>% \n    mutate(\n      `shimano dura-ace`        = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano dura-ace \") %>% as.numeric(),\n      `shimano ultegra`         = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano ultegra \") %>% as.numeric(),\n      `shimano 105`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano 105 \") %>% as.numeric(),\n      `shimano tiagra`          = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano tiagra \") %>% as.numeric(),\n      `Shimano sora`            = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano sora\") %>% as.numeric(),\n      `shimano deore`           = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano deore(?! xt)\") %>% as.numeric(),\n      `shimano slx`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano slx\") %>% as.numeric(),\n      `shimano grx`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano grx\") %>% as.numeric(),\n      `Shimano xt`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano deore xt |shimano xt \") %>% as.numeric(),\n      `Shimano xtr`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano xtr\") %>% as.numeric(),\n      `Shimano saint`           = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"shimano saint\") %>% as.numeric(),\n      `SRAM red`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram red\") %>% as.numeric(),\n      `SRAM force`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram force\") %>% as.numeric(),\n      `SRAM rival`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram rival\") %>% as.numeric(),\n      `SRAM apex`               = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram apex\") %>% as.numeric(),\n      `SRAM xx1`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram xx1\") %>% as.numeric(),\n      `SRAM x01`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram x01|sram xo1\") %>% as.numeric(),\n      `SRAM gx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram gx\") %>% as.numeric(),\n      `SRAM nx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram nx\") %>% as.numeric(),\n      `SRAM sx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram sx\") %>% as.numeric(),\n      `SRAM sx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"sram sx\") %>% as.numeric(),\n      `Campagnolo potenza`      = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"campagnolo potenza\") %>% as.numeric(),\n      `Campagnolo super record` = `Rear Derailleur` %>% str_to_lower() %>% str_detect(\"campagnolo super record\") %>% as.numeric(),\n      `shimano nexus`           = `Shift Lever`     %>% str_to_lower() %>% str_detect(\"shimano nexus\") %>% as.numeric(),\n      `shimano alfine`          = `Shift Lever`     %>% str_to_lower() %>% str_detect(\"shimano alfine\") %>% as.numeric()\n    ) %>% \n  # Remove original columns  \n  select(-c(`Rear Derailleur`, `Shift Lever`)) %>% \n  # Set all NAs to 0\n  mutate_if(is.numeric, ~replace(., is.na(.), 0))\n\n\n4 Order and tidy the tibble\n\n# 2.0 TRAINING & TEST SETS ----\nbike_features_tbl <- bike_features_tbl %>% \n  \n  mutate(id = row_number()) %>% \n  \n  select(id, everything(), -url)\n\n#Recipe : Challenge 2\n\n5 Step 1: Define the recipe\n\nbikes_rec <- \n  recipe(price ~ category_2 + frame_material, data = bike_orderlines_tbl) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_unknown() %>%\n  prep()\n\n\n6 Splitting the Data\n\nbike_features_tbl %>% distinct(category_2)\n\n\n\n  \n\n\n# run both following commands at the same time\nset.seed(seed = 1113)\nsplit_obj <- rsample::initial_split(bike_features_tbl, prop   = 0.80, \n                                                       strata = \"category_2\")\n\n# Check if testing contains all category_2 values\nsplit_obj %>% training() %>% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj %>% testing() %>% distinct(category_2)\n\n\n\n  \n\n\n# Assign training and test data\ntrain_tbl <- training(split_obj)\ntest_tbl  <- testing(split_obj)\n\n# We have to remove spaces and dashes from the column names\ntrain_tbl <- train_tbl %>% set_names(str_replace_all(names(train_tbl), \" |-\", \"_\"))\ntest_tbl  <- test_tbl  %>% set_names(str_replace_all(names(test_tbl),  \" |-\", \"_\"))\n\n#train_tbl$category_2 <- factor(train_tbl$category_2, levels = unique(bike_features_tbl$category_2))\n#test_tbl$category_2 <- factor(test_tbl$category_2, levels = unique(bike_features_tbl$category_2))\n\ntrain_transformed_tbl <- bake(bikes_rec, new_data = train_tbl) \ntest_transformed_tbl <- bake(bikes_rec, new_data = test_tbl) \n\n#Deleted E-Road, because there is no E-Road in the training_Tbl. Thus, it can't be trained on. (diff. levels)\n#Not Necessary, if used a recipe, due to \"step_unknown()\" function\n#test_tbl <- test_tbl[test_tbl$category_2 != \"E-Road\", ]\n\nbike_features_tbl\n\n\n\n  \n\n\n\n\n7 3.0 LINEAR METHODS —-\n\n8 3.1 LINEAR REGRESSION - NO ENGINEERED FEATURES —-\n\n# 3.1.1 Model ----\n\nmodel_01_linear_lm_simple <- linear_reg(mode = \"regression\") %>%\n    set_engine(\"lm\") #%>%\n # fit(price ~ category_2 + frame_material, data = train_tbl)\n\n\nbikes_wflow <- workflow() %>%\n  add_model(model_01_linear_lm_simple) %>%\n  add_recipe(bikes_rec)\nbikes_wflow\n\n#> ══ Workflow ════════════════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: linear_reg()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> 2 Recipe Steps\n#> \n#> • step_dummy()\n#> • step_unknown()\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: lm\n\nbikes_fit <-\n  bikes_wflow %>%\n  fit(data = train_tbl)\n\n#Predicting with a Recipe\n\nbikes_pred <-\n  predict(bikes_fit,test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\nbikes_pred\n\n\n\n  \n\n\n\n#Not Necessary anymore, because the prediction is done differently with a recipe\nmodel_01_linear_lm_simple %>% predict(new_data = test_tbl)\n#Calculating the common metrics comparing a “truth” (price) to an “estimate” (.pred).\n\n#model_01_linear_lm_simple %>%\n#  predict(new_data = test_tbl) %>%\nbikes_pred %>%\n\n    bind_cols(test_tbl %>% select(price)) %>%\n    \n    yardstick::metrics(truth = price, estimate = .pred)\n\n\n\n  \n\n\n\n#Extracting the Model of the Workflow\n\nmodel_01_linear_lm_simple <- bikes_fit %>% \n  pull_workflow_fit() #%>% \n\n#> Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\n#> ℹ Please use `extract_fit_parsnip()` instead.\n\n # tidy() \n\n\n9 Model Explanation\n\n10 3.1.2 Feature Importance —-\n\n#View(model_01_linear_lm_simple) # You will see the coefficients in the element \"fit\"\n\n# tidy() function is applicable for objects with class \"lm\"\n#model_01_linear_lm_simple %>% class()\n\nmodel_01_linear_lm_simple %>%\n  broom::tidy() %>%# NOT NECESSARY with a recipe, because while extracting the model, it gets tidied already\n  arrange(p.value) %>%\n  mutate(term = as_factor(term) %>% fct_rev()) %>%\n  ggplot(aes(x = estimate, y = term)) +\n  geom_point(color = \"#2dc6d6\", size = 3) +\n  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = \" €\", prefix = \"\")),\n                            size = 4, fill = \"#272A36\", color = \"white\") +\n  scale_x_continuous(labels = scales::dollar_format(suffix = \" €\", prefix = \"\")) +\n  labs(title = \"Linear Regression: Feature Importance\",\n       subtitle = \"Model 01: Simple lm Model\") \n\n\n\n\n\n\n\n\n11 3.1.3 Function to Calculate Metrics Without Recipe —-\n\n12 Code we used earlier\nmodel_01_linear_lm_simple %>% predict(new_data = test_tbl) %>%\nbind_cols(test_tbl %>% select(price)) %>%\nyardstick::metrics(truth = price, estimate = .pred)\n\n13 Generalized into a function\ncalc_metrics <- function(model, new_data = test_tbl) {\nmodel %>%\n    predict(new_data = new_data) %>%\n\n    bind_cols(new_data %>% select(price)) %>%\n    yardstick::metrics(truth = price, estimate = .pred)\n}\nmodel_01_linear_lm_simple %>% calc_metrics(test_tbl)\n#Function to Calculate Metrics With a Recipe\n\ncalc_metrics <- function(model, new_data = test_tbl) {\n\n    bikes_wflow <- workflow() %>%\n      add_model(model) %>%\n      add_recipe(bikes_rec)\n      bikes_wflow\n      \n    bikes_fit <-\n     bikes_wflow %>%\n     fit(data = train_tbl)\n    \n    bikes_pred <-\n     predict(bikes_fit,test_tbl)\n     bikes_pred\n     \n    bikes_pred %>%\n      bind_cols(test_tbl %>% select(price)) %>%\n      yardstick::metrics(truth = price, estimate = .pred)\n    \n    model <- \n     pull_workflow_fit(bikes_fit) \n}\n\n\n14 3.2 LINEAR REGRESSION - WITH ENGINEERED FEATURES —-\n\n# 3.2.1 Model ----\nmodel_02_linear_lm_complex <- linear_reg(\"regression\") %>%\n    set_engine(\"lm\")# %>%\n    \n    # This is going to be different. Remove unnecessary columns.\n  #  fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))\n\n#New Recipe (Preprocessing)\n\nbikes_rec <- \n  recipe(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender))) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_unknown(all_nominal())\n\nmodel_02_linear_lm_complex <- calc_metrics(model_02_linear_lm_complex, test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\n\n#> Warning in predict.lm(object = object$fit, newdata = new_data, type =\n#> \"response\"): Vorhersage durch Fit ohne vollen Rang mag täuschen\n\n\n\n15 3.2.2 Feature importance —-\n\nmodel_02_linear_lm_complex %>%\n  broom::tidy() %>%\n  arrange(p.value) %>%\n  mutate(term = as_factor(term) %>% fct_rev()) %>%\n  \n  ggplot(aes(x = estimate, y = term)) +\n  geom_point(color = \"#2dc6d6\", size = 3) +\n  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = \" €\", prefix = \"\")),\n                            size = 4, fill = \"#272A36\", color = \"white\") +\n  scale_x_continuous(labels = scales::dollar_format(suffix = \" €\", prefix = \"\")) +\n  labs(title = \"Linear Regression: Feature Importance\",\n       subtitle = \"Model 02: Complex lm Model\")\n\n#> Warning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n#> Warning: Removed 3 rows containing missing values (`geom_label_repel()`).\n\n\n\n\n\n\n\n\n\n16 3.3 PENALIZED REGRESSION —-\n\n# 3.3.1 Model ----\n\n\nmodel_03_linear_glmnet <- linear_reg(mode    = \"regression\", \n                                     penalty = 10, \n                                     mixture = 0.1) %>%\n    set_engine(\"glmnet\") \n   # fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))\n\n#model_03_linear_glmnet %>% calc_metrics(test_tbl)\n\n#New Recipe (Preprocessing)\n\nbikes_rec <- \n  recipe(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender))) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_unknown(all_nominal())\n\nmodel_03_linear_glmnet <- calc_metrics(model_03_linear_glmnet, test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\n\n\n# 3.3.2 Feature Importance ----\nmodel_03_linear_glmnet %>%\n    broom::tidy() %>%\n    #filter(lambda >= 10 & lambda < 11) %>%\n    \n    # No p value here\n    arrange(desc(abs(estimate))) %>%\n    mutate(term = as_factor(term) %>% fct_rev()) %>%\n\n    ggplot(aes(x = estimate, y = term)) +\n    geom_point() +\n    ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1)),\n                              size = 2) +\n    scale_x_continuous(labels = scales::dollar_format()) +\n    labs(title = \"Linear Regression: Feature Importance\",\n         subtitle = \"Model 03: GLMNET Model\")\n\n#> Lade nötiges Paket: Matrix\n\n\n#> \n#> Attache Paket: 'Matrix'\n\n\n#> Die folgenden Objekte sind maskiert von 'package:tidyr':\n#> \n#>     expand, pack, unpack\n\n\n#> Loaded glmnet 4.1-7\n\n\n\n\n\n\n\n\n\n# 4.0 TREE-BASED METHODS ----\n# 4.1 DECISION TREES ----\n# 4.1.1 Model ----\n\nmodel_04_tree_decision_tree <- decision_tree(mode = \"regression\",\n              \n              # Set the values accordingly to get started\n              cost_complexity = 0.001,\n              tree_depth      = 5,\n              min_n           = 7) %>%\n              \n    set_engine(\"rpart\")\n    #fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))\n\n#model_04_tree_decision_tree %>% calc_metrics(test_tbl)\nmodel_04_tree_decision_tree <- calc_metrics(model_04_tree_decision_tree, test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\n\n\n17 4.1.2 Decision Tree Plot —-\n\nmodel_04_tree_decision_tree$fit %>%\n    rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n# Optimze plot\nmodel_04_tree_decision_tree$fit %>%\n    rpart.plot(\n        roundint = FALSE,\n        type = 4,\n        extra = 101, # see help page\n        fallen.leaves = FALSE, # changes the angles from 90 to 45-degree\n        cex = 0.6, # font size\n        main = \"Model 04: Decision Tree\", # Adds title\n        box.palette = \"Blues\",\n        box.col = \"lightblue\"\n        )\n\n#> Warning: labs do not fit even at cex 0.15, there may be some overplotting\n\n\n\n\n\n\n\nshow.prp.palettes()\n\n\n\n\n\n\n\n\n18 4.3 XGBOOST —-\n\n19 4.3.1 Model —-\n\nset.seed(1234)\nmodel_07_boost_tree_xgboost <- boost_tree(\n    mode = \"regression\",\n    mtry = 30,\n    learn_rate = 0.25,\n    tree_depth = 7\n    ) %>%\n    set_engine(\"xgboost\")\n   # fit(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender)))\n\n#New Recipe (Preprocessing)\n\nbikes_rec <- \n  recipe(price ~ ., data = train_tbl %>% select(-c(id:weight), -category_1, -c(category_3:gender))) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_unknown(all_nominal())\n\nmodel_07_boost_tree_xgboost <- calc_metrics(model_07_boost_tree_xgboost, test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\n#model_07_boost_tree_xgboost %>% calc_metrics(test_tbl)\n\n\n20 4.3.2 Feature Importance —-\n\nmodel_07_boost_tree_xgboost$fit %>%\n    xgboost::xgb.importance(model = .) %>%\n    as_tibble() %>%\n    arrange(desc(Gain)) %>%\n    mutate(Feature = as_factor(Feature) %>% fct_rev()) %>%\n\n    ggplot(aes(Gain, Feature)) +\n    geom_point() +\n    labs(\n        title = \"XGBoost: Variable Importance\",\n        subtitle = \"Model 07: XGBoost Model\"\n    )"
  },
  {
    "objectID": "content/01_journal/Challenge3/Challenge3.html",
    "href": "content/01_journal/Challenge3/Challenge3.html",
    "title": "Challenge3",
    "section": "",
    "text": "library (h2o)\n\n#> \n#> ----------------------------------------------------------------------\n#> \n#> Your next step is to start H2O:\n#>     > h2o.init()\n#> \n#> For H2O package documentation, ask for help:\n#>     > ??h2o\n#> \n#> After starting H2O, you can use the Web UI at http://localhost:54321\n#> For more information visit https://docs.h2o.ai\n#> \n#> ----------------------------------------------------------------------\n\n\n#> \n#> Attache Paket: 'h2o'\n\n\n#> Die folgenden Objekte sind maskiert von 'package:stats':\n#> \n#>     cor, sd, var\n\n\n#> Die folgenden Objekte sind maskiert von 'package:base':\n#> \n#>     %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n#>     log10, log1p, log2, round, signif, trunc\n\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1\n\n\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ lubridate::day()   masks h2o::day()\n#> ✖ dplyr::filter()    masks stats::filter()\n#> ✖ lubridate::hour()  masks h2o::hour()\n#> ✖ dplyr::lag()       masks stats::lag()\n#> ✖ lubridate::month() masks h2o::month()\n#> ✖ lubridate::week()  masks h2o::week()\n#> ✖ lubridate::year()  masks h2o::year()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(readxl)\n\nlibrary(skimr)\n\nlibrary(GGally)\n\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2"
  },
  {
    "objectID": "content/01_journal/Challenge3/Challenge3.html#section-1",
    "href": "content/01_journal/Challenge3/Challenge3.html#section-1",
    "title": "Challenge3",
    "section": "\n0.2 ",
    "text": "0.2 \nYou can also embed plots, for example:\n\nh2o.init()\n\n#> \n#> H2O is not running yet, starting it now...\n#> \n#> Note:  In case of errors look at the following log files:\n#>     C:\\Users\\tiend\\AppData\\Local\\Temp\\RtmpKYXa2y\\file52241e194c91/h2o_tiend_started_from_r.out\n#>     C:\\Users\\tiend\\AppData\\Local\\Temp\\RtmpKYXa2y\\file5224f1f17ec/h2o_tiend_started_from_r.err\n#> \n#> \n#> Starting H2O JVM and connecting:  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 seconds 630 milliseconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.1 \n#>     H2O cluster version age:    3 months and 21 days \n#>     H2O cluster name:           H2O_started_from_R_tiend_ham688 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   3.99 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\n\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is (3 months and 21 days) old. There may be a newer version available.\n#> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\n\n\nhp_by_cyl <- mtcars %>% \n  group_by(cyl) %>%\n  summarize(min_hp=min(hp),\n            max_hp=max(hp))\nhp_by_cyl\n\n\n\n  \n\n\n\n\ngroupby_var <- quo(vs)\n\nhp_by_vs <- mtcars %>% \n              group_by(!!groupby_var) %>%\n              summarize(min_hp=min(hp),\n                        max_hp=max(hp))\nhp_by_vs\n\n\n\n  \n\n\n\n\ncar_stats <- function(groupby_var, measure_var) {\n\n    groupby_var <- enquo(groupby_var)\n    measure_var <- enquo(measure_var)\n    \n    ret <- mtcars %>% \n      \n             group_by(!!groupby_var) %>%\n             summarize(min = min(!!measure_var), max = max(!!measure_var)) %>%\n      \n             # Optional: as_label() and \"walrus operator\" :=\n             mutate(\n               measure_var = as_label(measure_var), !!measure_var := \"test\"\n               )\n    \n    return(ret)\n\n}\ncar_stats(am,hp)\n\n\n\n  \n\n\ncar_stats(gear,cyl)\n\n\n\n  \n\n\n\n\nscatter_plot <- function(data, x_var, y_var) {\n  \n  x_var <- enquo(x_var)\n  y_var <- enquo(y_var)\n  \n  ret <- data %>% \n           ggplot(aes(x = !!x_var, y = !!y_var)) + \n           geom_point() + \n           geom_smooth() +\n           ggtitle(str_c(as_label(y_var), \" vs. \",as_label(x_var)))\n \n  return(ret)\n}\nscatter_plot(mtcars, disp, hp)\n\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n#Business Case\n\nemployee_attrition_tbl <- read_csv(\"datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "content/01_journal/Challenge3/Challenge3.html#data-understanding",
    "href": "content/01_journal/Challenge3/Challenge3.html#data-understanding",
    "title": "Challenge3",
    "section": "\n5.1 2. Data Understanding",
    "text": "5.1 2. Data Understanding"
  },
  {
    "objectID": "content/01_journal/Challenge4/Challenge4.html",
    "href": "content/01_journal/Challenge4/Challenge4.html",
    "title": "Challenge4",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(readxl)\n\n# libraries\nlibrary(rsample)\nlibrary(recipes)\n\n#> \n#> Attache Paket: 'recipes'\n#> \n#> Das folgende Objekt ist maskiert 'package:stringr':\n#> \n#>     fixed\n#> \n#> Das folgende Objekt ist maskiert 'package:stats':\n#> \n#>     step\n\nlibrary(PerformanceAnalytics)  # for skewness \n\n#> Lade nötiges Paket: xts\n#> Lade nötiges Paket: zoo\n#> \n#> Attache Paket: 'zoo'\n#> \n#> Die folgenden Objekte sind maskiert von 'package:base':\n#> \n#>     as.Date, as.Date.numeric\n#> \n#> \n#> ######################### Warning from 'xts' package ##########################\n#> #                                                                             #\n#> # The dplyr lag() function breaks how base R's lag() function is supposed to  #\n#> # work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n#> # source() into this session won't work correctly.                            #\n#> #                                                                             #\n#> # Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n#> # conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n#> # dplyr from breaking base R's lag() function.                                #\n#> #                                                                             #\n#> # Code in packages is not affected. It's protected by R's namespace mechanism #\n#> # Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#> #                                                                             #\n#> ###############################################################################\n#> \n#> Attache Paket: 'xts'\n#> \n#> Die folgenden Objekte sind maskiert von 'package:dplyr':\n#> \n#>     first, last\n#> \n#> \n#> Attache Paket: 'PerformanceAnalytics'\n#> \n#> Das folgende Objekt ist maskiert 'package:graphics':\n#> \n#>     legend\n\n# H2O modeling\nlibrary(h2o)\n\n#> \n#> ----------------------------------------------------------------------\n#> \n#> Your next step is to start H2O:\n#>     > h2o.init()\n#> \n#> For H2O package documentation, ask for help:\n#>     > ??h2o\n#> \n#> After starting H2O, you can use the Web UI at http://localhost:54321\n#> For more information visit https://docs.h2o.ai\n#> \n#> ----------------------------------------------------------------------\n#> \n#> \n#> Attache Paket: 'h2o'\n#> \n#> Die folgenden Objekte sind maskiert von 'package:lubridate':\n#> \n#>     day, hour, month, week, year\n#> \n#> Die folgenden Objekte sind maskiert von 'package:stats':\n#> \n#>     cor, sd, var\n#> \n#> Die folgenden Objekte sind maskiert von 'package:base':\n#> \n#>     %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n#>     log10, log1p, log2, round, signif, trunc\n\n\n\n# Load data\n\n\nemployee_attrition_tbl <- read_csv(\"datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndefinitions_raw_tbl    <- read_excel(\"data_definitions.xlsx\", sheet = 1, col_names = FALSE)\n\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n\nView(definitions_raw_tbl)\n\n\nemployee_attrition_tbl %>% \n        ggplot(aes(Education)) +\n        geom_bar()"
  },
  {
    "objectID": "content/01_journal/Challenge5/Challenge5.html",
    "href": "content/01_journal/Challenge5/Challenge5.html",
    "title": "Challenge5",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\n#Load Libraries\n\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(readxl)\n\n# libraries\nlibrary(rsample)\nlibrary(recipes)\n\n#> \n#> Attache Paket: 'recipes'\n#> \n#> Das folgende Objekt ist maskiert 'package:stringr':\n#> \n#>     fixed\n#> \n#> Das folgende Objekt ist maskiert 'package:stats':\n#> \n#>     step\n\nlibrary(PerformanceAnalytics)  # for skewness \n\n#> Lade nötiges Paket: xts\n#> Lade nötiges Paket: zoo\n#> \n#> Attache Paket: 'zoo'\n#> \n#> Die folgenden Objekte sind maskiert von 'package:base':\n#> \n#>     as.Date, as.Date.numeric\n#> \n#> \n#> ######################### Warning from 'xts' package ##########################\n#> #                                                                             #\n#> # The dplyr lag() function breaks how base R's lag() function is supposed to  #\n#> # work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n#> # source() into this session won't work correctly.                            #\n#> #                                                                             #\n#> # Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n#> # conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n#> # dplyr from breaking base R's lag() function.                                #\n#> #                                                                             #\n#> # Code in packages is not affected. It's protected by R's namespace mechanism #\n#> # Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#> #                                                                             #\n#> ###############################################################################\n#> \n#> Attache Paket: 'xts'\n#> \n#> Die folgenden Objekte sind maskiert von 'package:dplyr':\n#> \n#>     first, last\n#> \n#> \n#> Attache Paket: 'PerformanceAnalytics'\n#> \n#> Das folgende Objekt ist maskiert 'package:graphics':\n#> \n#>     legend\n\n# H2O modeling\nlibrary(h2o)\n\n#> \n#> ----------------------------------------------------------------------\n#> \n#> Your next step is to start H2O:\n#>     > h2o.init()\n#> \n#> For H2O package documentation, ask for help:\n#>     > ??h2o\n#> \n#> After starting H2O, you can use the Web UI at http://localhost:54321\n#> For more information visit https://docs.h2o.ai\n#> \n#> ----------------------------------------------------------------------\n#> \n#> \n#> Attache Paket: 'h2o'\n#> \n#> Die folgenden Objekte sind maskiert von 'package:lubridate':\n#> \n#>     day, hour, month, week, year\n#> \n#> Die folgenden Objekte sind maskiert von 'package:stats':\n#> \n#>     cor, sd, var\n#> \n#> Die folgenden Objekte sind maskiert von 'package:base':\n#> \n#>     %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n#>     log10, log1p, log2, round, signif, trunc\n\n\n#H2o\n\nsource(\"process_hr_data_readable.R\")\n\nemployee_attrition_tbl          <- read_csv(\"datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndefinitions_raw_tbl             <- read_excel(\"data_definitions.xlsx\", sheet = 1, col_names = FALSE)\n\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n\nemployee_attrition_readable_tbl <- process_hr_data_readable(employee_attrition_tbl, definitions_raw_tbl)\n\n#> Joining with `by = join_by(Education)`\n#> Joining with `by = join_by(EnvironmentSatisfaction)`\n#> Joining with `by = join_by(JobInvolvement)`\n#> Joining with `by = join_by(JobSatisfaction)`\n#> Joining with `by = join_by(PerformanceRating)`\n#> Joining with `by = join_by(RelationshipSatisfaction)`\n#> Joining with `by = join_by(WorkLifeBalance)`\n\nset.seed(seed = 1113)\nsplit_obj                       <- rsample::initial_split(employee_attrition_readable_tbl, prop = 0.85)\ntrain_readable_tbl              <- training(split_obj)\ntest_readable_tbl               <- testing(split_obj)\n\nrecipe_obj <- recipe(Attrition ~., data = train_readable_tbl) %>% \n    step_zv(all_predictors()) %>% \n    step_mutate_at(JobLevel, StockOptionLevel, fn = as.factor) %>% \n    prep()\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)"
  },
  {
    "objectID": "content/01_journal/Challenge5/Challenge5.html#challenge-5-start--",
    "href": "content/01_journal/Challenge5/Challenge5.html#challenge-5-start--",
    "title": "Challenge5",
    "section": "\n6.1 - Challenge 5 Start -",
    "text": "6.1 - Challenge 5 Start -\n#1. Load the training & test dataset\n\nproduct_backorder_tbl          <- read_csv(\"product_backorders.csv\")\n\n#> Rows: 19053 Columns: 23\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (7): potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_bu...\n#> dbl (16): sku, national_inv, lead_time, in_transit_qty, forecast_3_month, fo...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n#2. Specifiy the response and predictor variables\n\nset.seed(seed = 1113)\nsplit_obj                       <- rsample::initial_split(product_backorder_tbl, prop = 0.85)\ntrain_readable_tbl              <- training(split_obj)\ntest_readable_tbl               <- testing(split_obj)\n\nrecipe_obj <- recipe(went_on_backorder ~., data = train_readable_tbl) %>% \n    step_zv(all_predictors()) %>% \n    prep()\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n\n#Modeling\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 hours 43 minutes \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.1 \n#>     H2O cluster version age:    3 months and 21 days \n#>     H2O cluster name:           H2O_started_from_R_tiend_ham688 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   2.97 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\n\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is (3 months and 21 days) old. There may be a newer version available.\n#> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\n# Split data into a training and a validation data frame\n# Setting the seed is just for reproducability\nsplit_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_tbl)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Set the target and predictors\ny <- \"went_on_backorder\"\nx <- setdiff(names(train_h2o), y)"
  },
  {
    "objectID": "content/01_journal/Challenge6/Challenge6.html",
    "href": "content/01_journal/Challenge6/Challenge6.html",
    "title": "Challenge6",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\n# LIME FEATURE EXPLANATION ----\n\n# 1. Setup ----\n\n# Load Libraries \n\nlibrary(h2o)\n\n#> \n#> ----------------------------------------------------------------------\n#> \n#> Your next step is to start H2O:\n#>     > h2o.init()\n#> \n#> For H2O package documentation, ask for help:\n#>     > ??h2o\n#> \n#> After starting H2O, you can use the Web UI at http://localhost:54321\n#> For more information visit https://docs.h2o.ai\n#> \n#> ----------------------------------------------------------------------\n\n\n#> \n#> Attache Paket: 'h2o'\n\n\n#> Die folgenden Objekte sind maskiert von 'package:stats':\n#> \n#>     cor, sd, var\n\n\n#> Die folgenden Objekte sind maskiert von 'package:base':\n#> \n#>     %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n#>     log10, log1p, log2, round, signif, trunc\n\nlibrary(recipes)\n\n#> Lade nötiges Paket: dplyr\n\n\n#> \n#> Attache Paket: 'dplyr'\n\n\n#> Die folgenden Objekte sind maskiert von 'package:stats':\n#> \n#>     filter, lag\n\n\n#> Die folgenden Objekte sind maskiert von 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\n\n#> \n#> Attache Paket: 'recipes'\n\n\n#> Das folgende Objekt ist maskiert 'package:stats':\n#> \n#>     step\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ forcats   1.0.0     ✔ readr     2.1.4\n#> ✔ ggplot2   3.4.2     ✔ stringr   1.5.0\n#> ✔ lubridate 1.9.2     ✔ tibble    3.2.1\n#> ✔ purrr     1.0.1     ✔ tidyr     1.3.0\n\n\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ lubridate::day()   masks h2o::day()\n#> ✖ dplyr::filter()    masks stats::filter()\n#> ✖ stringr::fixed()   masks recipes::fixed()\n#> ✖ lubridate::hour()  masks h2o::hour()\n#> ✖ dplyr::lag()       masks stats::lag()\n#> ✖ lubridate::month() masks h2o::month()\n#> ✖ lubridate::week()  masks h2o::week()\n#> ✖ lubridate::year()  masks h2o::year()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(tidyquant)\n\n#> Lade nötiges Paket: PerformanceAnalytics\n#> Lade nötiges Paket: xts\n#> Lade nötiges Paket: zoo\n#> \n#> Attache Paket: 'zoo'\n#> \n#> Die folgenden Objekte sind maskiert von 'package:base':\n#> \n#>     as.Date, as.Date.numeric\n#> \n#> \n#> ######################### Warning from 'xts' package ##########################\n#> #                                                                             #\n#> # The dplyr lag() function breaks how base R's lag() function is supposed to  #\n#> # work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n#> # source() into this session won't work correctly.                            #\n#> #                                                                             #\n#> # Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n#> # conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n#> # dplyr from breaking base R's lag() function.                                #\n#> #                                                                             #\n#> # Code in packages is not affected. It's protected by R's namespace mechanism #\n#> # Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#> #                                                                             #\n#> ###############################################################################\n#> \n#> Attache Paket: 'xts'\n#> \n#> Die folgenden Objekte sind maskiert von 'package:dplyr':\n#> \n#>     first, last\n#> \n#> \n#> Attache Paket: 'PerformanceAnalytics'\n#> \n#> Das folgende Objekt ist maskiert 'package:graphics':\n#> \n#>     legend\n#> \n#> Lade nötiges Paket: quantmod\n#> Lade nötiges Paket: TTR\n#> Registered S3 method overwritten by 'quantmod':\n#>   method            from\n#>   as.zoo.data.frame zoo\n\nlibrary(lime)\n\n#> \n#> Attache Paket: 'lime'\n#> \n#> Das folgende Objekt ist maskiert 'package:dplyr':\n#> \n#>     explain\n\nlibrary(rsample)\nlibrary(ggplot2)\n\n# Load Data\nemployee_attrition_tbl <- read_csv(\"datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndefinitions_raw_tbl    <- read_excel(\"data_definitions.xlsx\", sheet = 1, col_names = FALSE)\n\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n\n# Processing Pipeline\nsource(\"process_hr_data_readable.R\")\n\nemployee_attrition_readable_tbl <- process_hr_data_readable(employee_attrition_tbl, definitions_raw_tbl)\n\n#> Joining with `by = join_by(Education)`\n#> Joining with `by = join_by(EnvironmentSatisfaction)`\n#> Joining with `by = join_by(JobInvolvement)`\n#> Joining with `by = join_by(JobSatisfaction)`\n#> Joining with `by = join_by(PerformanceRating)`\n#> Joining with `by = join_by(RelationshipSatisfaction)`\n#> Joining with `by = join_by(WorkLifeBalance)`\n\n# Split into test and train\nset.seed(seed = 1113)\nsplit_obj <- rsample::initial_split(employee_attrition_readable_tbl, prop = 0.85)\n\n# Assign training and test data\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl  <- testing(split_obj)\n\n# ML Preprocessing Recipe \nrecipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%\n                step_zv(all_predictors()) %>%\n                step_mutate_at(c(\"JobLevel\", \"StockOptionLevel\"), fn = as.factor) %>% \n                prep()\n\nrecipe_obj\n\n#> \n#> ── Recipe ──────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs \n#> Number of variables by role\n#> outcome:    1\n#> predictor: 34\n#> \n#> ── Training information \n#> Training data contained 1249 data points and no incomplete rows.\n#> \n#> ── Operations \n#> • Zero variance filter removed: EmployeeCount, Over18, StandardHours | Trained\n#> • Variable mutation for: JobLevel, StockOptionLevel | Trained\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n\n# 2. Models ----\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 hours 47 minutes \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.1 \n#>     H2O cluster version age:    3 months and 21 days \n#>     H2O cluster name:           H2O_started_from_R_tiend_ham688 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   2.94 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\n\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is (3 months and 21 days) old. There may be a newer version available.\n#> Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\nautoml_leader <- h2o.loadModel(\"StackedEnsemble_BestOfFamily_3_AutoML_2_20230525_211824\")\nautoml_leader\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_BestOfFamily_3_AutoML_2_20230525_211824 \n#> Model Summary for Stacked Ensemble: \n#>                                          key            value\n#> 1                          Stacking strategy cross_validation\n#> 2       Number of base models (used / total)              3/5\n#> 3           # GBM base models (used / total)              1/1\n#> 4           # GLM base models (used / total)              1/1\n#> 5  # DeepLearning base models (used / total)              1/1\n#> 6           # DRF base models (used / total)              0/2\n#> 7                      Metalearner algorithm              GLM\n#> 8         Metalearner fold assignment scheme           Random\n#> 9                         Metalearner nfolds                5\n#> 10                   Metalearner fold_column               NA\n#> 11        Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.06548413\n#> RMSE:  0.2558987\n#> LogLoss:  0.2304629\n#> Mean Per-Class Error:  0.1707158\n#> AUC:  0.9220945\n#> AUCPR:  0.8093744\n#> Gini:  0.8441889\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error      Rate\n#> No     886  23 0.025303   =23/909\n#> Yes     49 106 0.316129   =49/155\n#> Totals 935 129 0.067669  =72/1064\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.319522   0.746479 109\n#> 2                       max f2  0.153294   0.765247 190\n#> 3                 max f0point5  0.418566   0.801105  82\n#> 4                 max accuracy  0.319522   0.932331 109\n#> 5                max precision  0.954868   1.000000   0\n#> 6                   max recall  0.007825   1.000000 384\n#> 7              max specificity  0.954868   1.000000   0\n#> 8             max absolute_mcc  0.319522   0.711784 109\n#> 9   max min_per_class_accuracy  0.148133   0.864516 194\n#> 10 max mean_per_class_accuracy  0.153294   0.865226 190\n#> 11                     max tns  0.954868 909.000000   0\n#> 12                     max fns  0.954868 154.000000   0\n#> 13                     max fps  0.000832 909.000000 399\n#> 14                     max tps  0.007825 155.000000 384\n#> 15                     max tnr  0.954868   1.000000   0\n#> 16                     max fnr  0.954868   0.993548   0\n#> 17                     max fpr  0.000832   1.000000 399\n#> 18                     max tpr  0.007825   1.000000 384\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.1044121\n#> RMSE:  0.3231286\n#> LogLoss:  0.340833\n#> Mean Per-Class Error:  0.1889545\n#> AUC:  0.8728965\n#> AUCPR:  0.7327814\n#> Gini:  0.7457931\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error     Rate\n#> No     134  13 0.088435  =13/147\n#> Yes     11  27 0.289474   =11/38\n#> Totals 145  40 0.129730  =24/185\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.257768   0.692308  39\n#> 2                       max f2  0.221863   0.725000  47\n#> 3                 max f0point5  0.375262   0.719178  26\n#> 4                 max accuracy  0.375262   0.875676  26\n#> 5                max precision  0.909310   1.000000   0\n#> 6                   max recall  0.018888   1.000000 139\n#> 7              max specificity  0.909310   1.000000   0\n#> 8             max absolute_mcc  0.257768   0.610507  39\n#> 9   max min_per_class_accuracy  0.156669   0.789116  60\n#> 10 max mean_per_class_accuracy  0.221863   0.816953  47\n#> 11                     max tns  0.909310 147.000000   0\n#> 12                     max fns  0.909310  37.000000   0\n#> 13                     max fps  0.000908 147.000000 184\n#> 14                     max tps  0.018888  38.000000 139\n#> 15                     max tnr  0.909310   1.000000   0\n#> 16                     max fnr  0.909310   0.973684   0\n#> 17                     max fpr  0.000908   1.000000 184\n#> 18                     max tpr  0.018888   1.000000 139\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.08582408\n#> RMSE:  0.2929575\n#> LogLoss:  0.3007471\n#> Mean Per-Class Error:  0.2284254\n#> AUC:  0.8401789\n#> AUCPR:  0.6047858\n#> Gini:  0.6803577\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error       Rate\n#> No     828  81 0.089109    =81/909\n#> Yes     57  98 0.367742    =57/155\n#> Totals 885 179 0.129699  =138/1064\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.273443   0.586826 140\n#> 2                       max f2  0.207638   0.644599 176\n#> 3                 max f0point5  0.481961   0.636550  71\n#> 4                 max accuracy  0.481961   0.892857  71\n#> 5                max precision  0.961951   1.000000   0\n#> 6                   max recall  0.002828   1.000000 397\n#> 7              max specificity  0.961951   1.000000   0\n#> 8             max absolute_mcc  0.390134   0.516054  93\n#> 9   max min_per_class_accuracy  0.160560   0.774194 213\n#> 10 max mean_per_class_accuracy  0.207638   0.786557 176\n#> 11                     max tns  0.961951 909.000000   0\n#> 12                     max fns  0.961951 154.000000   0\n#> 13                     max fps  0.001325 909.000000 399\n#> 14                     max tps  0.002828 155.000000 397\n#> 15                     max tnr  0.961951   1.000000   0\n#> 16                     max fnr  0.961951   0.993548   0\n#> 17                     max fpr  0.001325   1.000000 399\n#> 18                     max tpr  0.002828   1.000000 397\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                mean       sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy   0.903100 0.019689   0.879167   0.932773   0.894472   0.901478\n#> auc        0.851520 0.024984   0.821533   0.880376   0.873540   0.835510\n#> err        0.096900 0.019689   0.120833   0.067227   0.105528   0.098522\n#> err_count 20.600000 5.128353  29.000000  16.000000  21.000000  20.000000\n#> f0point5   0.676573 0.038446   0.628571   0.698925   0.722543   0.646552\n#>           cv_5_valid\n#> accuracy    0.907609\n#> auc         0.846641\n#> err         0.092391\n#> err_count  17.000000\n#> f0point5    0.686275\n#> \n#> ---\n#>                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#> precision           0.713144  0.047590   0.647059   0.764706   0.735294\n#> r2                  0.309256  0.021469   0.297469   0.317660   0.338736\n#> recall              0.566791  0.062890   0.564103   0.520000   0.675676\n#> residual_deviance 127.260010 20.341250 160.076320 112.791030 132.083560\n#> rmse                0.292059  0.024484   0.309209   0.253270   0.316368\n#> specificity         0.958864  0.016910   0.940299   0.981221   0.944444\n#>                   cv_4_valid cv_5_valid\n#> precision           0.681818   0.736842\n#> r2                  0.281639   0.310776\n#> recall              0.535714   0.538462\n#> residual_deviance 121.883100 109.466020\n#> rmse                0.292263   0.289186\n#> specificity         0.960000   0.968354"
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  }
]